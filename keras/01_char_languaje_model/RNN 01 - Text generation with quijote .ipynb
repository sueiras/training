{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a RNN model to text generation\n",
    "- RNN model at character level\n",
    "    - Input: n character previous\n",
    "    - Output: next character\n",
    "    - Model LSTM\n",
    "- Use 'El Quijote' to train the generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Header\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "path = '/home/ubuntu/data/training/keras/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data and generate sequences\n",
    "\n",
    "Download quijote from guttenberg project\n",
    "\n",
    "\n",
    "wget http://www.gutenberg.org/cache/epub/2000/pg2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2117498\n",
      "Chars list:  ['\\n', ' ', '!', '\"', '#', '$', '%', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '»', '¿', 'à', 'á', 'é', 'í', 'ï', 'ñ', 'ó', 'ù', 'ú', 'ü', '\\ufeff']\n",
      "total chars: 72\n"
     ]
    }
   ],
   "source": [
    "#Read book\n",
    "text = open(path + \"pg2000.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('Chars list: ', chars)\n",
    "print('total chars:', len(chars))\n",
    "\n",
    "#Dictionaries to convert char to num & num to char\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 705726\n",
      "tregará a medea; si  - d\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "# One sentence of length 20 for each 3 characters\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(300, len(text) - maxlen, step): #Start in line 30 to exclude Gutenberg header.\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "print(sentences[4996], '-', next_chars[4996])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "X shape:  (705726, 20, 72)\n",
      "y shape:  (705726, 72)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X: One row by sentence\n",
    "    in each row a matrix of bool 0/1 of dim length_sentence x num_chars coding the sentence. Dummy variables\n",
    "y: One row by sentence\n",
    "    in each row a vector of bool of lengt num_chars with 1 in the next char position\n",
    "'''\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('y shape: ',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)`\n",
      "  '` call to the Keras 2 API: ' + signature)\n",
      "/home/jorge/anaconda3/envs/keras2_py36/lib/python3.6/site-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)`\n",
      "  '` call to the Keras 2 API: ' + signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prev (InputLayer)            (None, 20, 72)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 512)           1198080   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 72)                36936     \n",
      "=================================================================\n",
      "Total params: 3,334,216.0\n",
      "Trainable params: 3,334,216.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model: 2 stacked LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, LSTM\n",
    "\n",
    "\n",
    "print('Build model 1')\n",
    "seq_prev_input = Input(shape=(maxlen, len(chars)), name='prev') \n",
    "                \n",
    "# apply forwards LSTM\n",
    "forwards1 = LSTM(512, dropout_W=0.3, dropout_U=0.3, return_sequences=True)(seq_prev_input)\n",
    "\n",
    "forwards2 = LSTM(512, dropout_W=0.3, dropout_U=0.3, return_sequences=False)(forwards1)\n",
    "\n",
    "output = Dense(len(chars), activation='softmax')(forwards2)\n",
    "\n",
    "model1 = Model(input=seq_prev_input, output=output)\n",
    "model1.summary()\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"268pt\" viewBox=\"0.00 0.00 146.00 268.00\" width=\"146pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 264)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-264 142,-264 142,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140185202959056 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140185202959056</title>\n",
       "<polygon fill=\"none\" points=\"0,-223 0,-259 138,-259 138,-223 0,-223\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-237.3\">prev: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140187267510736 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140187267510736</title>\n",
       "<polygon fill=\"none\" points=\"11,-149 11,-185 127,-185 127,-149 11,-149\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-163.3\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140185202959056&#45;&gt;140187267510736 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140185202959056-&gt;140187267510736</title>\n",
       "<path d=\"M69,-222.937C69,-214.807 69,-204.876 69,-195.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"72.5001,-195.441 69,-185.441 65.5001,-195.441 72.5001,-195.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140186996324176 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140186996324176</title>\n",
       "<polygon fill=\"none\" points=\"11,-75 11,-111 127,-111 127,-75 11,-75\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-89.3\">lstm_2: LSTM</text>\n",
       "</g>\n",
       "<!-- 140187267510736&#45;&gt;140186996324176 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140187267510736-&gt;140186996324176</title>\n",
       "<path d=\"M69,-148.937C69,-140.807 69,-130.876 69,-121.705\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"72.5001,-121.441 69,-111.441 65.5001,-121.441 72.5001,-121.441\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140185202347536 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140185202347536</title>\n",
       "<polygon fill=\"none\" points=\"5,-1 5,-37 133,-37 133,-1 5,-1\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-15.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140186996324176&#45;&gt;140185202347536 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140186996324176-&gt;140185202347536</title>\n",
       "<path d=\"M69,-74.937C69,-66.8072 69,-56.8761 69,-47.7047\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"72.5001,-47.4406 69,-37.4407 65.5001,-47.4407 72.5001,-47.4406\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the model\n",
    "#Plot the model graph\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model1).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600000 samples, validate on 132869 samples\n",
      "Epoch 1/30\n",
      "600000/600000 [==============================] - 317s - loss: 2.2843 - acc: 0.3188 - val_loss: 1.9308 - val_acc: 0.4116\n",
      "Epoch 2/30\n",
      "600000/600000 [==============================] - 315s - loss: 1.8326 - acc: 0.4273 - val_loss: 1.7618 - val_acc: 0.4673\n",
      "Epoch 3/30\n",
      "600000/600000 [==============================] - 314s - loss: 1.6826 - acc: 0.4730 - val_loss: 1.6641 - val_acc: 0.4964\n",
      "Epoch 4/30\n",
      "600000/600000 [==============================] - 314s - loss: 1.9580 - acc: 0.4019 - val_loss: 2.0847 - val_acc: 0.3665\n",
      "Epoch 5/30\n",
      "600000/600000 [==============================] - 314s - loss: 2.0036 - acc: 0.3775 - val_loss: 1.9042 - val_acc: 0.4214\n",
      "Epoch 6/30\n",
      "600000/600000 [==============================] - 315s - loss: 1.8555 - acc: 0.4199 - val_loss: 1.8052 - val_acc: 0.4512\n",
      "Epoch 7/30\n",
      "570368/600000 [===========================>..] - ETA: 14s - loss: 1.7612 - acc: 0.4488"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "history = model1.fit(X[:600000], y[:600000], batch_size=512, nb_epoch=2,\n",
    "           validation_data=(X[600000:], y[600000:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train on 600000 samples, validate on 132869 samples\n",
    "Epoch 1/30\n",
    "600000/600000 [==============================] - 523s - loss: 2.2971 - acc: 0.3154 - val_loss: 1.9305 - val_acc: 0.4111\n",
    "Epoch 2/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.8107 - acc: 0.4357 - val_loss: 1.7215 - val_acc: 0.4805\n",
    "Epoch 3/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.6351 - acc: 0.4875 - val_loss: 1.6226 - val_acc: 0.5123\n",
    "Epoch 4/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.5340 - acc: 0.5168 - val_loss: 1.5604 - val_acc: 0.5306\n",
    "Epoch 5/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4689 - acc: 0.5361 - val_loss: 1.5244 - val_acc: 0.5443\n",
    "Epoch 6/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.4232 - acc: 0.5494 - val_loss: 1.5053 - val_acc: 0.5508\n",
    "Epoch 7/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3890 - acc: 0.5596 - val_loss: 1.4823 - val_acc: 0.5579\n",
    "Epoch 8/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3618 - acc: 0.5672 - val_loss: 1.4643 - val_acc: 0.5625\n",
    "Epoch 9/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.3409 - acc: 0.5730 - val_loss: 1.4587 - val_acc: 0.5667\n",
    "Epoch 10/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.3231 - acc: 0.5784 - val_loss: 1.4466 - val_acc: 0.5690\n",
    "Epoch 11/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.3084 - acc: 0.5819 - val_loss: 1.4398 - val_acc: 0.5736\n",
    "Epoch 12/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2954 - acc: 0.5864 - val_loss: 1.4360 - val_acc: 0.5740\n",
    "Epoch 13/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2839 - acc: 0.5885 - val_loss: 1.4296 - val_acc: 0.5776\n",
    "Epoch 14/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2751 - acc: 0.5913 - val_loss: 1.4275 - val_acc: 0.5785\n",
    "Epoch 15/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2645 - acc: 0.5944 - val_loss: 1.4230 - val_acc: 0.5794\n",
    "Epoch 16/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2578 - acc: 0.5963 - val_loss: 1.4182 - val_acc: 0.5816\n",
    "Epoch 17/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2508 - acc: 0.5988 - val_loss: 1.4202 - val_acc: 0.5818\n",
    "Epoch 18/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2455 - acc: 0.6005 - val_loss: 1.4151 - val_acc: 0.5812\n",
    "Epoch 19/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2394 - acc: 0.6017 - val_loss: 1.4133 - val_acc: 0.5836\n",
    "Epoch 20/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2332 - acc: 0.6037 - val_loss: 1.4099 - val_acc: 0.5848\n",
    "Epoch 21/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2288 - acc: 0.6051 - val_loss: 1.4164 - val_acc: 0.5837\n",
    "Epoch 22/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2247 - acc: 0.6057 - val_loss: 1.4102 - val_acc: 0.5861\n",
    "Epoch 23/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.2215 - acc: 0.6067 - val_loss: 1.4080 - val_acc: 0.5873\n",
    "Epoch 24/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2167 - acc: 0.6078 - val_loss: 1.4099 - val_acc: 0.5855\n",
    "Epoch 25/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2124 - acc: 0.6097 - val_loss: 1.4050 - val_acc: 0.5876\n",
    "Epoch 26/30\n",
    "600000/600000 [==============================] - 522s - loss: 1.2111 - acc: 0.6101 - val_loss: 1.4105 - val_acc: 0.5868\n",
    "Epoch 27/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2059 - acc: 0.6116 - val_loss: 1.4027 - val_acc: 0.5879\n",
    "Epoch 28/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.2026 - acc: 0.6122 - val_loss: 1.4048 - val_acc: 0.5868\n",
    "Epoch 29/30\n",
    "600000/600000 [==============================] - 523s - loss: 1.1990 - acc: 0.6138 - val_loss: 1.4090 - val_acc: 0.5887\n",
    "Epoch 30/30\n",
    "600000/600000 [==============================] - 524s - loss: 1.1969 - acc: 0.6139 - val_loss: 1.4053 - val_acc: 0.5889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-422066002d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'figure.figsize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save model\n",
    "'''\n",
    "model_name = 'text_generation_model1'\n",
    "\n",
    "json_string = model1.to_json()\n",
    "open(path + 'models/mdl_' + model_name + '.json', 'w').write(json_string)\n",
    "model1.save_weights(path + 'models/w_' + model_name + '.h5')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model_name = 'text_generation_model1'\n",
    "\n",
    "model1 = model_from_json(open(path + 'models/mdl_' + model_name + '.json').read())\n",
    "model1.load_weights(path + 'models/w_' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "\n",
    "\n",
    "def sample(a, diversity=1.0):\n",
    "    '''\n",
    "    helper function to sample an index from a probability array\n",
    "    - Diversity control the level of randomless\n",
    "    '''\n",
    "    a = np.log(a) / diversity\n",
    "    a = np.exp(a) / np.sum(np.exp(a), axis=0)\n",
    "    a /= np.sum(a+0.0000001) #Precission error\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "\n",
    "def generate_text(sentence, diversity, current_model, num_char=400):\n",
    "    sentence_init = sentence\n",
    "    generated = ''\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = current_model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "    print()\n",
    "    print('DIVERSITY: ',diversity)\n",
    "    print(sentence_init + generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/py27/lib/python2.7/site-packages/ipykernel/__main__.py:9: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIVERSITY:  0.2\n",
      "mire vuestra merced lo ha de ser mi señor don quijote -dijo el cura-, que estaba en la mitad del caballero de la mano, y le dijo:\n",
      "\n",
      "-¿qué dios se le diga en la mitad del mundo, y el caballero del caballero de la mano de los de los demás de la mujer de la mano, que está en el mundo los ojos de la mano, y que la había de ser el deseo de su caballero andante, y el cura que le dijese la mano de la mano, y de la ma\n",
      "\n",
      "DIVERSITY:  0.5\n",
      "mire vuestra merced cuando le dijo:\n",
      "\n",
      "-esto se le había de hacer más de los ojos de su grande aventura, con mucha falta de los río, y el tal se puedo preguntar por los\n",
      "tres más propósitos de los dos las comedimientos de los dos de los de la vida, arrojó las alforjas de los ojos de su amo, el cual, como lo cual visto en el suelo. pero no sé qué tiempos que con los de los dos pastores con toda la cabeza de la\n",
      "\n",
      "DIVERSITY:  1\n",
      "mire vuestra merced a los amorosos que\n",
      "sabes:\n",
      "\n",
      "todos estos en el mundo que a caballero andante como los pensamientos de los escuderos; según lo menos, murmurono -replicó don quijote-; y, dándole más determinaciones ni pensamientos y grandes temerosos bondanes de mi padre de su mano a ella con dársele cuán gileo no le\n",
      "hallaré dos herederos estoy dijo:\n",
      "\n",
      "-uno miraba, goloquio con que impresas, inaunitas com\n",
      "\n",
      "DIVERSITY:  1.2\n",
      "mire vuestra merced pagase lotario desde el temerle, que tienen por allí a los negocios\n",
      "alientos.\n",
      "\n",
      "-esá entiendo yo que tengo\n",
      "todo pulto, me dieron y mayores\n",
      "judaban ser moros, del nombre no hay pala las que cargaten bien los\n",
      "unos hubiesen resuntotido.\n",
      "\n",
      "-con todo aquello otro cuantos vos habían dicho, siempre ayudaría aquella jaez, y no ha de determinar tiene; mi otra tendrá ya la duquesa, donde le dio m\n",
      "\n",
      "DIVERSITY:  0.2\n",
      "de lo que sucedió a su amo, y el cura que el caballero del mundo. en lo que dijo el cura, y el cura que el caballero de la mano de la cabeza, y el cura que el caballero de la mano, sino que en los demás de la mano de la mano, que no se le diga que en la cabeza de la mano, y el cura que en la mitad del caballero de la mano de la mano de la mano de la cabeza, y el cura que le dijo a su amo, y aun a los demás de los \n",
      "\n",
      "DIVERSITY:  0.5\n",
      "de lo que sucedió al caballero de la cabeza, y los demás personajes de los dos de los amorosos que ellas no se ha de comparar a su caballero, sino al lugar de las mujeres y de los reyes, que no se hacen y contra un caballero del cura y desta manera me ha de ser de la mano, sino al caballero de la cabeza, y de que todos los dos menos de la comparable con un lugar de los ojos, que le dijo:\n",
      "\n",
      "-a lo que respondió que\n",
      "\n",
      "DIVERSITY:  1\n",
      "de lo que sucedió a leonera: don quijote\n",
      "\n",
      "   muy diocumpro en la cabeza, que los cuadrilleros andantes en la tal hijos, caballeros andantes en la memoria, sino\n",
      "palabras que pasadas de la orden de su numbre, mandayo a una manera por cosesisión, como puedo enviendo de coroas, y que, entre los trabajotes apartados, y el juicio, finca que se representa de lo que puede hacer la mar. el\n",
      "cura y aun encerrados por\n",
      "el \n",
      "\n",
      "DIVERSITY:  1.2\n",
      "de lo que sucedió a\n",
      "la cabeza.\n",
      "\n",
      "-y -ni encubre el labrador confesó por la jarr, y vieron\n",
      "en camila; y, ciéndole luego, pero no sé la miración, que\n",
      "yo sé qué mi mal desnudachén; y con el cabo le\n",
      "volviese mucho sus madres otras cuantas ejurcicios le\n",
      "odejas.\n",
      "\n",
      "-busca salió de aquellos sobre esta trabajo.\n",
      "\n",
      "-y, volviéndose, asimismo no pudienno valentísimamente quedaron ha quedaba, puesto en sus servi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = 'mire vuestra merced '\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'de lo que sucedió a'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'de allí a poco come'\n",
    "generate_text(sentence, 0.2, model1)\n",
    "generate_text(sentence, 0.5, model1)\n",
    "generate_text(sentence, 1,   model1)\n",
    "generate_text(sentence, 1.2, model1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "mire vuestra merced decís, y que se le pareció que en la cabeza de la cabeza, y el caballero del caballero de la cabeza, y los demás de los demás de los demás de los demás de la mano de la mujer de la mano de la mano, y aun a su señora dulcinea del toboso, y el cura que el caballero de la mano que le pareció que estaba en la misma cosa que en la mitad del caballero de la mano, se le dijo:\n",
    "\n",
    "-no sé -respondi\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "mire vuestra merced que no está en la cabeza. pero, en efeto, pues todo aquello que está en el mundo que cada uno debía\n",
    "de ser con la misma sierra parte de la muerte de la mano, con todo el mundo me la conoció, que tenía por el primero que están el rostro en las manos, y al señor don quijote -respondió sancho-, porque en la puerta de los sucesos del buen estado de la desencantada de los ojos.\n",
    "\n",
    "-¿qué mal \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "mire vuestra merced que, aunque, desdicho y tiempo viene, a\n",
    "cuya cabeza destos tres, los informaciones que dejara de serle, que me fueron? ¿admirado,\n",
    "\n",
    "y, creyendo que me va y enfermo. y esto que tienen entonces las requiebros que algunos limpios en un ampeoso como si improvisentes en sus insimulables y en los míos al que el honesto, en la mano de mucho premio y don quijote fingióno los dos o sabidores, y,\n",
    "acom\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "mire vuestra merced paso que no sa cluero-. subió, señor,\n",
    "le hubiera vuelto el duque, que pica, yo fue pose�ría de guardar cierto. para los demás, esperando la misma tragua debe de haberlas hallado su santa hijo ni debían para mí, porque yo hay de platar pre subir tú turba -dijo el deleitable-; otras porturas, sino como alabanzas y\n",
    "comedimientos puciese\n",
    "los días de lo que von mirado después de\n",
    "visión de\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de lo que sucedió a la mano de la mano de la mano de los ojos de la mano de la mano de la mano, y aun más que se le dijese la mano de la mano y en la mitad del caballero del caballero de la mano de la mano, y que el parte de la mano, y que en la mitad del caballero de la mano, sino a la puerta de la mano de la mano de la mano de los demás de la cabeza, y el cura y la cabeza de la mano y en la cabeza de la cabeza, \n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de lo que sucedió a camila, por ser tan alta de la mano y en voz brazo? ¿qué es lo que pudiera, señor don quijote que en las fermosuras que después que le sacarán con don quijote había de ser la duquesa, la cual no le había de ser muy buena como gausa, y si este deseo de ser mejor que en la cabeza está a los dos de la mano, y su amo no le puede dar a su casa por los manos de mano, diciendo:\n",
    "\n",
    "-pues, ¿qué \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de lo que sucedió a las más faltas cuentan haciendo:\n",
    "\n",
    "-�qcorrían por esto, bueno -respondió cómo está aquel mano traía furia por ella estajo\n",
    "más que andaba y fingión de sus apartieron de modo que no\n",
    "hay vasto qué hizo con galdáis que soy sin duda, duque ni en la más\n",
    "buena gran señora dulcinea; y así lo han don quijote y no bastaba junto a nuestra\n",
    "señora dulcinea, ahora\n",
    "la entienda, el aposento a\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de lo que sucedió al\n",
    "admiración, dijo:\n",
    "\n",
    "-eso me esplevo en razón encantada, y su venimo tiempo, que pasaba tan hirtoria. es\n",
    "\n",
    "don quijote, le dijo:\n",
    "\n",
    "y cuando jamás:\n",
    "  si ya entienden en mi padre desde aquí vean fuerza, como yo\n",
    "costa yo he oído decir, el rey\n",
    "\n",
    "vención que ésto, que lo més comenzó a vuestra merced colgaré por la orden y parece que\n",
    "pudieron semplarse. el cual, si de la mono.\n",
    "acudi�\n",
    "('\\n\\nDIVERSITY: ', 0.2, '\\n')\n",
    "de allí a poco comenzó a su caballero andante, que en la más hermosa de la mano, y el cura que el mundo tenía con el cura y el de la mano de la cabeza de la mano de la mano de la mano de la mano de la cabeza, y a los demás de los cuatro de la mano de la mano de la mano y en la mitad del caballero de la cabeza, y le dijo:\n",
    "\n",
    "-¡oh sancho -dijo el cura-, que no se le habían de ser manos de los demás de los de la\n",
    "('\\n\\nDIVERSITY: ', 0.5, '\\n')\n",
    "de allí a poco comenzó a la vida de la mano de\n",
    "arriba al mundo. pero, con todo eso, ha de ser el rostro de las linajes de su escudero. por el rey en el mundo de la industria que en su caballero andante; y, aunque se volvió a camila y de la mano de la muerte de mi cabeza? y así, como el tal caballero andante, que los demás juramentos, y con la mitad del toboso, y así, por decir que os son de allí a los deseos \n",
    "('\\n\\nDIVERSITY: ', 1, '\\n')\n",
    "de allí a poco comenzó\n",
    "a dos crazos de entender que tratar la nueva al desde nuevas andantes de solos con los detros donde me hubieran\n",
    "de subir la sumiera y rabia la duquesa\n",
    "   a un hacer con un gato, que le\n",
    "dijo juntar la locura para su\n",
    "amo, para que quisieron decir alguna, vino\n",
    "todas aquellos demás caballeros. pero, sancho, tanto, viendo camila la primero tiene: prodición\n",
    "que yo le dio caminar las belloz\n",
    "('\\n\\nDIVERSITY: ', 1.2, '\\n')\n",
    "de allí a poco comer. otros días se le pidía hablar de cerra\n",
    "   que el gate predice\n",
    "y otras puntas del cordel bestia.\n",
    "\n",
    "-yo no por eso, decaría sobre la venta, porque él saslos demás sin él que\n",
    "llevase a buscar de la suma, sancho milático, cuando posían los zogados. y si así, mi rendido a\n",
    "cuerpo, ni en llopar salió el baece-, que el hábísimo que viene por\n",
    "los tratas y tontos que sean, y el de los"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
