{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq model for chatbot\n",
    "    - Direct adaptation of the seq2seq tensorflow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Binary for training translation models and decoding from them.\n",
    "\n",
    "Running this program without --decode will download the WMT corpus into\n",
    "the directory specified as --data_dir and tokenize it in a very basic way,\n",
    "and then start training a model saving checkpoints to --train_dir.\n",
    "\n",
    "Running with --decode starts an interactive loop so you can see how\n",
    "the current checkpoint translates English sentences into French.\n",
    "\n",
    "See the following papers for more information on neural translation models.\n",
    " * http://arxiv.org/abs/1409.3215\n",
    " * http://arxiv.org/abs/1409.0473\n",
    " * http://arxiv.org/abs/1412.2007\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "\n",
    "FLAGS_learning_rate = 0.5            # Learning rate.\n",
    "FLAGS_learning_rate_decay_factor = 0.99\n",
    "FLAGS_max_gradient_norm = 5.0\n",
    "FLAGS_batch_size = 16\n",
    "FLAGS_size = 512                 #Size of each model layer.\n",
    "FLAGS_num_layers = 3              # Number of layers in the model.\n",
    "FLAGS_en_vocab_size = 40000       # En vocabulary size.\n",
    "FLAGS_fr_vocab_size = 40000       # French vocabulary size.\n",
    "FLAGS_data_dir = \"/tmp/chatbot/data\"           # Data directory\")\n",
    "FLAGS_train_dir = \"/tmp/chatbot/models\"          # Training directory.\")\n",
    "FLAGS_max_train_data_size = 0     # Limit on the size of training data (0: no limit).\n",
    "FLAGS_steps_per_checkpoint = 200  # How many training steps to do per checkpoint.\n",
    "FLAGS_decode = False      # Set to True for interactive decoding.\n",
    "FLAGS_self_test = False   # Run a self-test if this is set to True.\n",
    "FLAGS_use_fp16 = False    # Train using fp16 instead of fp32.\n",
    "\n",
    "\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "import tensorflow as tf\n",
    "\n",
    "# Special vocabulary symbols - we always put them at the start.\n",
    "_PAD = b\"_PAD\"\n",
    "_GO = b\"_GO\"\n",
    "_EOS = b\"_EOS\"\n",
    "_UNK = b\"_UNK\"\n",
    "_START_VOCAB = [_PAD, _GO, _EOS, _UNK]\n",
    "\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "# Regular expressions used to tokenize.\n",
    "_WORD_SPLIT = re.compile(b\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(br\"\\d\")\n",
    "\n",
    "# URLs for WMT data.\n",
    "_WMT_ENFR_TRAIN_URL = \"http://www.statmt.org/wmt10/training-giga-fren.tar\"\n",
    "_WMT_ENFR_DEV_URL = \"http://www.statmt.org/wmt15/dev-v2.tgz\"\n",
    "\n",
    "\n",
    "def maybe_download(directory, filename, url):\n",
    "    \"\"\"Download filename from url unless it's already in directory.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(\"Creating directory %s\" % directory)\n",
    "        os.mkdir(directory)\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"Downloading %s to %s\" % (url, filepath))\n",
    "        filepath, _ = urllib.request.urlretrieve(url, filepath)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print(\"Successfully downloaded\", filename, statinfo.st_size, \"bytes\")\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def gunzip_file(gz_path, new_path):\n",
    "    \"\"\"Unzips from gz_path into new_path.\"\"\"\n",
    "    print(\"Unpacking %s to %s\" % (gz_path, new_path))\n",
    "    with gzip.open(gz_path, \"rb\") as gz_file:\n",
    "        with open(new_path, \"wb\") as new_file:\n",
    "            for line in gz_file:\n",
    "                new_file.write(line)\n",
    "\n",
    "\n",
    "def get_wmt_enfr_train_set(directory):\n",
    "    \"\"\"Download the WMT en-fr training corpus to directory unless it's there.\"\"\"\n",
    "    train_path = os.path.join(directory, \"giga-fren.release2.fixed\")\n",
    "    if not (gfile.Exists(train_path +\".fr\") and gfile.Exists(train_path +\".en\")):\n",
    "        corpus_file = maybe_download(directory, \"training-giga-fren.tar\",\n",
    "                                     _WMT_ENFR_TRAIN_URL)\n",
    "        print(\"Extracting tar file %s\" % corpus_file)\n",
    "        with tarfile.open(corpus_file, \"r\") as corpus_tar:\n",
    "            corpus_tar.extractall(directory)\n",
    "        gunzip_file(train_path + \".fr.gz\", train_path + \".fr\")\n",
    "        gunzip_file(train_path + \".en.gz\", train_path + \".en\")\n",
    "    return train_path\n",
    "\n",
    "\n",
    "def get_wmt_enfr_dev_set(directory):\n",
    "    \"\"\"Download the WMT en-fr training corpus to directory unless it's there.\"\"\"\n",
    "    dev_name = \"newstest2013\"\n",
    "    dev_path = os.path.join(directory, dev_name)\n",
    "    if not (gfile.Exists(dev_path + \".fr\") and gfile.Exists(dev_path + \".en\")):\n",
    "        dev_file = maybe_download(directory, \"dev-v2.tgz\", _WMT_ENFR_DEV_URL)\n",
    "        print(\"Extracting tgz file %s\" % dev_file)\n",
    "        with tarfile.open(dev_file, \"r:gz\") as dev_tar:\n",
    "            fr_dev_file = dev_tar.getmember(\"dev/\" + dev_name + \".fr\")\n",
    "            en_dev_file = dev_tar.getmember(\"dev/\" + dev_name + \".en\")\n",
    "            fr_dev_file.name = dev_name + \".fr\"  # Extract without \"dev/\" prefix.\n",
    "            en_dev_file.name = dev_name + \".en\"\n",
    "            dev_tar.extract(fr_dev_file, directory)\n",
    "            dev_tar.extract(en_dev_file, directory)\n",
    "    return dev_path\n",
    "\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "\n",
    "def create_vocabulary(vocabulary_path, data_path, max_vocabulary_size,\n",
    "                      tokenizer=None, normalize_digits=True):\n",
    "    \"\"\"Create vocabulary file (if it does not exist yet) from data file.\n",
    "\n",
    "    Data file is assumed to contain one sentence per line. Each sentence is\n",
    "    tokenized and digits are normalized (if normalize_digits is set).\n",
    "    Vocabulary contains the most-frequent tokens up to max_vocabulary_size.\n",
    "    We write it to vocabulary_path in a one-token-per-line format, so that later\n",
    "    token in the first line gets id=0, second line gets id=1, and so on.\n",
    "\n",
    "    Args:\n",
    "        vocabulary_path: path where the vocabulary will be created.\n",
    "        data_path: data file that will be used to create vocabulary.\n",
    "        max_vocabulary_size: limit on the size of the created vocabulary.\n",
    "        tokenizer: a function to use to tokenize each data sentence;\n",
    "            if None, basic_tokenizer will be used.\n",
    "        normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "    \"\"\"\n",
    "    if not gfile.Exists(vocabulary_path):\n",
    "        print(\"Creating vocabulary %s from data %s\" % (vocabulary_path, data_path))\n",
    "        vocab = {}\n",
    "        with gfile.GFile(data_path, mode=\"rb\") as f:\n",
    "            counter = 0\n",
    "            for line in f:\n",
    "                counter += 1\n",
    "                if counter % 100000 == 0:\n",
    "                    print(\"  processing line %d\" % counter)\n",
    "                line = tf.compat.as_bytes(line)\n",
    "                tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n",
    "                for w in tokens:\n",
    "                    word = _DIGIT_RE.sub(b\"0\", w) if normalize_digits else w\n",
    "                    if word in vocab:\n",
    "                        vocab[word] += 1\n",
    "                    else:\n",
    "                        vocab[word] = 1\n",
    "            vocab_list = _START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "            if len(vocab_list) > max_vocabulary_size:\n",
    "                vocab_list = vocab_list[:max_vocabulary_size]\n",
    "            with gfile.GFile(vocabulary_path, mode=\"wb\") as vocab_file:\n",
    "                for w in vocab_list:\n",
    "                    vocab_file.write(w + b\"\\n\")\n",
    "\n",
    "\n",
    "def initialize_vocabulary(vocabulary_path):\n",
    "    \"\"\"Initialize vocabulary from file.\n",
    "\n",
    "    We assume the vocabulary is stored one-item-per-line, so a file:\n",
    "      dog\n",
    "      cat\n",
    "    will result in a vocabulary {\"dog\": 0, \"cat\": 1}, and this function will\n",
    "    also return the reversed-vocabulary [\"dog\", \"cat\"].\n",
    "\n",
    "    Args:\n",
    "      vocabulary_path: path to the file containing the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "      a pair: the vocabulary (a dictionary mapping string to integers), and\n",
    "      the reversed vocabulary (a list, which reverses the vocabulary mapping).\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if the provided vocabulary_path does not exist.\n",
    "    \"\"\"\n",
    "    if gfile.Exists(vocabulary_path):\n",
    "        rev_vocab = []\n",
    "        with gfile.GFile(vocabulary_path, mode=\"rb\") as f:\n",
    "            rev_vocab.extend(f.readlines())\n",
    "        rev_vocab = [line.strip() for line in rev_vocab]\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        raise ValueError(\"Vocabulary file %s not found.\", vocabulary_path)\n",
    "\n",
    "\n",
    "def sentence_to_token_ids(sentence, vocabulary,\n",
    "                          tokenizer=None, normalize_digits=True):\n",
    "    \"\"\"Convert a string to list of integers representing token-ids.\n",
    "\n",
    "    For example, a sentence \"I have a dog\" may become tokenized into\n",
    "    [\"I\", \"have\", \"a\", \"dog\"] and with vocabulary {\"I\": 1, \"have\": 2,\n",
    "    \"a\": 4, \"dog\": 7\"} this function will return [1, 2, 4, 7].\n",
    "\n",
    "    Args:\n",
    "      sentence: the sentence in bytes format to convert to token-ids.\n",
    "      vocabulary: a dictionary mapping tokens to integers.\n",
    "      tokenizer: a function to use to tokenize each sentence;\n",
    "        if None, basic_tokenizer will be used.\n",
    "      normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "\n",
    "    Returns:\n",
    "      a list of integers, the token-ids for the sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    if tokenizer:\n",
    "        words = tokenizer(sentence)\n",
    "    else:\n",
    "        words = basic_tokenizer(sentence)\n",
    "    if not normalize_digits:\n",
    "        return [vocabulary.get(w, UNK_ID) for w in words]\n",
    "    # Normalize digits by 0 before looking words up in the vocabulary.\n",
    "    return [vocabulary.get(_DIGIT_RE.sub(b\"0\", w), UNK_ID) for w in words]\n",
    "\n",
    "\n",
    "def data_to_token_ids(data_path, target_path, vocabulary_path,\n",
    "                      tokenizer=None, normalize_digits=True):\n",
    "    \"\"\"Tokenize data file and turn into token-ids using given vocabulary file.\n",
    "\n",
    "    This function loads data line-by-line from data_path, calls the above\n",
    "    sentence_to_token_ids, and saves the result to target_path. See comment\n",
    "    for sentence_to_token_ids on the details of token-ids format.\n",
    "\n",
    "    Args:\n",
    "      data_path: path to the data file in one-sentence-per-line format.\n",
    "      target_path: path where the file with token-ids will be created.\n",
    "      vocabulary_path: path to the vocabulary file.\n",
    "      tokenizer: a function to use to tokenize each sentence;\n",
    "        if None, basic_tokenizer will be used.\n",
    "      normalize_digits: Boolean; if true, all digits are replaced by 0s.\n",
    "    \"\"\"\n",
    "    if not gfile.Exists(target_path):\n",
    "        print(\"Tokenizing data in %s\" % data_path)\n",
    "        vocab, _ = initialize_vocabulary(vocabulary_path)\n",
    "        with gfile.GFile(data_path, mode=\"rb\") as data_file:\n",
    "            with gfile.GFile(target_path, mode=\"w\") as tokens_file:\n",
    "                counter = 0\n",
    "                for line in data_file:\n",
    "                    counter += 1\n",
    "                    if counter % 100000 == 0:\n",
    "                        print(\"  tokenizing line %d\" % counter)\n",
    "                    token_ids = sentence_to_token_ids(tf.compat.as_bytes(line), vocab,\n",
    "                                                      tokenizer, normalize_digits)\n",
    "                    tokens_file.write(\" \".join([str(tok) for tok in token_ids]) + \"\\n\")\n",
    "\n",
    "\n",
    "def prepare_wmt_data(data_dir, en_vocabulary_size, fr_vocabulary_size, tokenizer=None):\n",
    "    \"\"\"Get WMT data into data_dir, create vocabularies and tokenize data.\n",
    "\n",
    "    Args:\n",
    "        data_dir: directory in which the data sets will be stored.\n",
    "        en_vocabulary_size: size of the English vocabulary to create and use.\n",
    "        fr_vocabulary_size: size of the French vocabulary to create and use.\n",
    "        tokenizer: a function to use to tokenize each data sentence;\n",
    "            if None, basic_tokenizer will be used.\n",
    "\n",
    "    Returns:\n",
    "      A tuple of 6 elements:\n",
    "        (1) path to the token-ids for English training data-set,\n",
    "        (2) path to the token-ids for French training data-set,\n",
    "        (3) path to the token-ids for English development data-set,\n",
    "        (4) path to the token-ids for French development data-set,\n",
    "        (5) path to the English vocabulary file,\n",
    "        (6) path to the French vocabulary file.\n",
    "    \"\"\"\n",
    "    # Get wmt data to the specified directory.\n",
    "    train_path = get_wmt_enfr_train_set(data_dir)\n",
    "    dev_path = get_wmt_enfr_dev_set(data_dir)\n",
    "\n",
    "    # Create vocabularies of the appropriate sizes.\n",
    "    fr_vocab_path = os.path.join(data_dir, \"vocab%d.fr\" % fr_vocabulary_size)\n",
    "    en_vocab_path = os.path.join(data_dir, \"vocab%d.en\" % en_vocabulary_size)\n",
    "    create_vocabulary(fr_vocab_path, train_path + \".fr\", fr_vocabulary_size, tokenizer)\n",
    "    create_vocabulary(en_vocab_path, train_path + \".en\", en_vocabulary_size, tokenizer)\n",
    "\n",
    "    # Create token ids for the training data.\n",
    "    fr_train_ids_path = train_path + (\".ids%d.fr\" % fr_vocabulary_size)\n",
    "    en_train_ids_path = train_path + (\".ids%d.en\" % en_vocabulary_size)\n",
    "    data_to_token_ids(train_path + \".fr\", fr_train_ids_path, fr_vocab_path, tokenizer)\n",
    "    data_to_token_ids(train_path + \".en\", en_train_ids_path, en_vocab_path, tokenizer)\n",
    "\n",
    "    # Create token ids for the development data.\n",
    "    fr_dev_ids_path = dev_path + (\".ids%d.fr\" % fr_vocabulary_size)\n",
    "    en_dev_ids_path = dev_path + (\".ids%d.en\" % en_vocabulary_size)\n",
    "    data_to_token_ids(dev_path + \".fr\", fr_dev_ids_path, fr_vocab_path, tokenizer)\n",
    "    data_to_token_ids(dev_path + \".en\", en_dev_ids_path, en_vocab_path, tokenizer)\n",
    "\n",
    "    return (en_train_ids_path, fr_train_ids_path,\n",
    "            en_dev_ids_path, fr_dev_ids_path,\n",
    "            en_vocab_path, fr_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specifit data management\n",
    "# - Objective: 4 files with the enc_trn, dec_trn, enc_tst, & dec_tst sentences\n",
    "#\n",
    "# Read subtitles and create trian 6 test lines\n",
    "#\n",
    "data_source_path = '/home/jorge/data/training/text/' \n",
    "\n",
    "# Generator of list of files in a folder and subfolders\n",
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "\n",
    "def gen_find(filepattern, toppath):\n",
    "    '''\n",
    "    Generator with a recursive list of files in the toppath that match filepattern \n",
    "    Inputs:\n",
    "        filepattern(str): Command stype pattern \n",
    "        toppath(str): Root path\n",
    "    '''\n",
    "    for path, dirlist, filelist in os.walk(toppath):\n",
    "        for name in fnmatch.filter(filelist, filepattern):\n",
    "            yield os.path.join(path, name)\n",
    "\n",
    "\n",
    "train_files = gen_find(\"*[1-9]raw.txt\", data_source_path+'OpenSubtitlesRaw/')\n",
    "test_files = gen_find(\"*0raw.txt\", data_source_path+'OpenSubtitlesRaw/')\n",
    "\n",
    "def read_sentences(generator):\n",
    "    sentences = []\n",
    "    for ff in generator:\n",
    "        with open(ff, 'r') as f:\n",
    "            sentences += f.read().splitlines()\n",
    "    return sentences        \n",
    "\n",
    "train_sentences = read_sentences(train_files)\n",
    "test_sentences = read_sentences(test_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save sentences \n",
    "with open('/tmp/chatbot/data/trn_sentences.en', \"wb\") as enc_file:\n",
    "    with open('/tmp/chatbot/data/trn_sentences.de', \"wb\") as dec_file:\n",
    "        for i in range(len(train_sentences)-1):\n",
    "            enc_file.write(train_sentences[i]+'\\n')\n",
    "            dec_file.write(train_sentences[i+1]+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save sentences \n",
    "with open('/tmp/chatbot/data/tst_sentences.en', \"wb\") as enc_file:\n",
    "    with open('/tmp/chatbot/data/tst_sentences.de', \"wb\") as dec_file:\n",
    "        for i in range(len(test_sentences)-1):\n",
    "            enc_file.write(test_sentences[i]+'\\n')\n",
    "            dec_file.write(test_sentences[i+1]+'\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_wmt_data(data_dir, en_vocabulary_size, fr_vocabulary_size, tokenizer=None):\n",
    "\n",
    "\n",
    "    train_path = os.path.join(data_dir, \"trn_sentences\")\n",
    "    dev_path = os.path.join(data_dir, \"tst_sentences\")\n",
    "    \n",
    "    \n",
    "    # Create vocabularies of the appropriate sizes.\n",
    "    fr_vocab_path = os.path.join(data_dir, \"vocab%d.de\" % fr_vocabulary_size)\n",
    "    en_vocab_path = os.path.join(data_dir, \"vocab%d.en\" % en_vocabulary_size)\n",
    "    create_vocabulary(fr_vocab_path, train_path + \".de\", fr_vocabulary_size, tokenizer)\n",
    "    create_vocabulary(en_vocab_path, train_path + \".en\", en_vocabulary_size, tokenizer)\n",
    "\n",
    "    # Create token ids for the training data.\n",
    "    fr_train_ids_path = train_path + (\".ids%d.de\" % fr_vocabulary_size)\n",
    "    en_train_ids_path = train_path + (\".ids%d.en\" % en_vocabulary_size)\n",
    "    data_to_token_ids(train_path + \".de\", fr_train_ids_path, fr_vocab_path, tokenizer)\n",
    "    data_to_token_ids(train_path + \".en\", en_train_ids_path, en_vocab_path, tokenizer)\n",
    "\n",
    "    # Create token ids for the development data.\n",
    "    fr_dev_ids_path = dev_path + (\".ids%d.de\" % fr_vocabulary_size)\n",
    "    en_dev_ids_path = dev_path + (\".ids%d.en\" % en_vocabulary_size)\n",
    "    data_to_token_ids(dev_path + \".de\", fr_dev_ids_path, fr_vocab_path, tokenizer)\n",
    "    data_to_token_ids(dev_path + \".en\", en_dev_ids_path, en_vocab_path, tokenizer)\n",
    "\n",
    "    return (en_train_ids_path, fr_train_ids_path,\n",
    "            en_dev_ids_path, fr_dev_ids_path,\n",
    "            en_vocab_path, fr_vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "\n",
    "\n",
    "class Seq2SeqModel(object):\n",
    "    \"\"\"Sequence-to-sequence model with attention and for multiple buckets.\n",
    "\n",
    "    This class implements a multi-layer recurrent neural network as encoder,\n",
    "    and an attention-based decoder. This is the same as the model described in\n",
    "    this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n",
    "    or into the seq2seq library for complete model implementation.\n",
    "    This class also allows to use GRU cells in addition to LSTM cells, and\n",
    "    sampled softmax to handle large output vocabulary size. A single-layer\n",
    "    version of this model, but with bi-directional encoder, was presented in\n",
    "        http://arxiv.org/abs/1409.0473\n",
    "    and sampled softmax is described in Section 3 of the following paper.\n",
    "        http://arxiv.org/abs/1412.2007\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 source_vocab_size,\n",
    "                 target_vocab_size,\n",
    "                 buckets,\n",
    "                 size,\n",
    "                 num_layers,\n",
    "                 max_gradient_norm,\n",
    "                 batch_size,\n",
    "                 learning_rate,\n",
    "                 learning_rate_decay_factor,\n",
    "                 use_lstm=False,\n",
    "                 num_samples=512,\n",
    "                 forward_only=False,\n",
    "                 dtype=tf.float32):\n",
    "        \"\"\"Create the model.\n",
    "  \n",
    "        Args:\n",
    "          source_vocab_size: size of the source vocabulary.\n",
    "          target_vocab_size: size of the target vocabulary.\n",
    "          buckets: a list of pairs (I, O), where I specifies maximum input length\n",
    "              that will be processed in that bucket, and O specifies maximum output\n",
    "              length. Training instances that have inputs longer than I or outputs\n",
    "              longer than O will be pushed to the next bucket and padded accordingly.\n",
    "              We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n",
    "          size: number of units in each layer of the model.\n",
    "          num_layers: number of layers in the model.\n",
    "          max_gradient_norm: gradients will be clipped to maximally this norm.\n",
    "          batch_size: the size of the batches used during training;\n",
    "              the model construction is independent of batch_size, so it can be\n",
    "              changed after initialization if this is convenient, e.g., for decoding.\n",
    "          learning_rate: learning rate to start with.\n",
    "          learning_rate_decay_factor: decay learning rate by this much when needed.\n",
    "          use_lstm: if true, we use LSTM cells instead of GRU cells.\n",
    "          num_samples: number of samples for sampled softmax.\n",
    "          forward_only: if set, we do not construct the backward pass in the model.\n",
    "          dtype: the data type to use to store internal variables.\n",
    "        \"\"\"\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.buckets = buckets\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(\n",
    "            float(learning_rate), trainable=False, dtype=dtype)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # If we use sampled softmax, we need an output projection.\n",
    "        output_projection = None\n",
    "        softmax_loss_function = None\n",
    "        # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "        if num_samples > 0 and num_samples < self.target_vocab_size:\n",
    "            w_t = tf.get_variable(\"proj_w\", [self.target_vocab_size, size], dtype=dtype)\n",
    "            w = tf.transpose(w_t)\n",
    "            b = tf.get_variable(\"proj_b\", [self.target_vocab_size], dtype=dtype)\n",
    "            output_projection = (w, b)\n",
    "\n",
    "            def sampled_loss(labels, inputs):\n",
    "                labels = tf.reshape(labels, [-1, 1])\n",
    "                # We need to compute the sampled_softmax_loss using 32bit floats to\n",
    "                # avoid numerical instabilities.\n",
    "                local_w_t = tf.cast(w_t, tf.float32)\n",
    "                local_b = tf.cast(b, tf.float32)\n",
    "                local_inputs = tf.cast(inputs, tf.float32)\n",
    "                return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,\n",
    "                                             num_samples, self.target_vocab_size), dtype)\n",
    "            softmax_loss_function = sampled_loss\n",
    "\n",
    "        # Create the internal multi-layer cell for our RNN.\n",
    "        single_cell = tf.contrib.rnn.GRUCell(size)\n",
    "        if use_lstm:\n",
    "            single_cell = tf.contrib.rnn.BasicLSTMCell(size)\n",
    "        cell = single_cell\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "        # The seq2seq function: we use embedding for the input and attention.\n",
    "        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "            return tf.nn.seq2seq.embedding_attention_seq2seq(\n",
    "                encoder_inputs,\n",
    "                decoder_inputs,\n",
    "                cell,\n",
    "                num_encoder_symbols=source_vocab_size,\n",
    "                num_decoder_symbols=target_vocab_size,\n",
    "                embedding_size=size,\n",
    "                output_projection=output_projection,\n",
    "                feed_previous=do_decode,\n",
    "                dtype=dtype)\n",
    "\n",
    "        # Feeds for inputs.\n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.target_weights = []\n",
    "        for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                    name=\"encoder{0}\".format(i)))\n",
    "        for i in xrange(buckets[-1][1] + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                    name=\"decoder{0}\".format(i)))\n",
    "            self.target_weights.append(tf.placeholder(dtype, shape=[None],\n",
    "                                                    name=\"weight{0}\".format(i)))\n",
    " \n",
    "        # Our targets are decoder inputs shifted by one.\n",
    "        targets = [self.decoder_inputs[i + 1]\n",
    "                   for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "\n",
    "        # Training outputs and losses.\n",
    "        if forward_only:\n",
    "            self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
    "                self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n",
    "                softmax_loss_function=softmax_loss_function)\n",
    "            # If we use output projection, we need to project outputs for decoding.\n",
    "            if output_projection is not None:\n",
    "                for b in xrange(len(buckets)):\n",
    "                    self.outputs[b] = [\n",
    "                        tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "                        for output in self.outputs[b]\n",
    "                    ]\n",
    "        else:\n",
    "            self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(\n",
    "                self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                self.target_weights, buckets,\n",
    "                lambda x, y: seq2seq_f(x, y, False),\n",
    "                softmax_loss_function=softmax_loss_function)\n",
    "\n",
    "        # Gradients and SGD update operation for training the model.\n",
    "        params = tf.trainable_variables()\n",
    "        if not forward_only:\n",
    "            self.gradient_norms = []\n",
    "            self.updates = []\n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "            for b in xrange(len(buckets)):\n",
    "                gradients = tf.gradients(self.losses[b], params)\n",
    "                clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                               max_gradient_norm)\n",
    "                self.gradient_norms.append(norm)\n",
    "                self.updates.append(opt.apply_gradients(\n",
    "                    zip(clipped_gradients, params), global_step=self.global_step))\n",
    "\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "             bucket_id, forward_only):\n",
    "        \"\"\"Run a step of the model feeding the given inputs.\n",
    "\n",
    "        Args:\n",
    "          session: tensorflow session to use.\n",
    "          encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n",
    "          decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n",
    "          target_weights: list of numpy float vectors to feed as target weights.\n",
    "          bucket_id: which bucket of the model to use.\n",
    "          forward_only: whether to do the backward step or only forward.\n",
    "\n",
    "        Returns:\n",
    "          A triple consisting of gradient norm (or None if we did not do backward),\n",
    "          average perplexity, and the outputs.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: if length of encoder_inputs, decoder_inputs, or\n",
    "            target_weights disagrees with bucket size for the specified bucket_id.\n",
    "        \"\"\"\n",
    "        # Check if the sizes match.\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        if len(encoder_inputs) != encoder_size:\n",
    "            raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
    "                             \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
    "        if len(decoder_inputs) != decoder_size:\n",
    "            raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
    "                             \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
    "        if len(target_weights) != decoder_size:\n",
    "            raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
    "                             \" %d != %d.\" % (len(target_weights), decoder_size))\n",
    "\n",
    "        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "        input_feed = {}\n",
    "        for l in xrange(encoder_size):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "        for l in xrange(decoder_size):\n",
    "            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "            input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "        # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "        last_target = self.decoder_inputs[decoder_size].name\n",
    "        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "        # Output feed: depends on whether we do a backward step or not.\n",
    "        if not forward_only:\n",
    "            output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                           self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                           self.losses[bucket_id]]  # Loss for this batch.\n",
    "        else:\n",
    "            output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
    "            for l in xrange(decoder_size):  # Output logits.\n",
    "                output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        if not forward_only:\n",
    "            return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "        else:\n",
    "            return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_batch(self, data, bucket_id):\n",
    "        \"\"\"Get a random batch of data from the specified bucket, prepare for step.\n",
    "\n",
    "        To feed data in step(..) it must be a list of batch-major vectors, while\n",
    "        data here contains single length-major cases. So the main logic of this\n",
    "        function is to re-index data cases to be in the proper format for feeding.\n",
    "\n",
    "        Args:\n",
    "          data: a tuple of size len(self.buckets) in which each element contains\n",
    "            lists of pairs of input and output data that we use to create a batch.\n",
    "          bucket_id: integer, which bucket to get the batch for.\n",
    "\n",
    "        Returns:\n",
    "          The triple (encoder_inputs, decoder_inputs, target_weights) for\n",
    "          the constructed batch that has the proper format to call step(...) later.\n",
    "        \"\"\"\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "        # Get a random batch of encoder and decoder inputs from data,\n",
    "        # pad them if needed, reverse encoder inputs and add GO to decoder.\n",
    "        for _ in xrange(self.batch_size):\n",
    "            encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "\n",
    "            # Encoder inputs are padded and then reversed.\n",
    "            encoder_pad = [PAD_ID] * (encoder_size - len(encoder_input))\n",
    "            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "            # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "            decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "            decoder_inputs.append([GO_ID] + decoder_input +\n",
    "                                  [PAD_ID] * decoder_pad_size)\n",
    "\n",
    "        # Now we create batch-major vectors from the data selected above.\n",
    "        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "        # Batch encoder inputs are just re-indexed encoder_inputs.\n",
    "        for length_idx in xrange(encoder_size):\n",
    "            batch_encoder_inputs.append(\n",
    "                np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                          for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "        # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
    "        for length_idx in xrange(decoder_size):\n",
    "            batch_decoder_inputs.append(\n",
    "                np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                          for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "            # Create target_weights to be 0 for targets that are padding.\n",
    "            batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
    "            for batch_idx in xrange(self.batch_size):\n",
    "                # We set weight to 0 if the corresponding target is a PAD symbol.\n",
    "                # The corresponding target is decoder_input shifted by 1 forward.\n",
    "                if length_idx < decoder_size - 1:\n",
    "                    target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "                if length_idx == decoder_size - 1 or target == PAD_ID:\n",
    "                    batch_weight[batch_idx] = 0.0\n",
    "            batch_weights.append(batch_weight)\n",
    "        \n",
    "        return batch_encoder_inputs, batch_decoder_inputs, batch_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(source_path, target_path, max_size=None):\n",
    "    \"\"\"Read data from source and target files and put into buckets.\n",
    "\n",
    "    Args:\n",
    "      source_path: path to the files with token-ids for the source language.\n",
    "      target_path: path to the file with token-ids for the target language;\n",
    "          it must be aligned with the source file: n-th line contains the desired\n",
    "          output for n-th line from the source_path.\n",
    "      max_size: maximum number of lines to read, all other will be ignored;\n",
    "          if 0 or None, data files will be read completely (no limit).\n",
    "\n",
    "    Returns:\n",
    "        data_set: a list of length len(_buckets); data_set[n] contains a list of\n",
    "            (source, target) pairs read from the provided data files that fit\n",
    "            into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n",
    "            len(target) < _buckets[n][1]; source and target are lists of token-ids.\n",
    "    \"\"\"\n",
    "    data_set = [[] for _ in _buckets]\n",
    "    with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "        with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "            source, target = source_file.readline(), target_file.readline()\n",
    "            counter = 0\n",
    "            while source and target and (not max_size or counter < max_size):\n",
    "                counter += 1\n",
    "                if counter % 100000 == 0:\n",
    "                    print(\"  reading data line %d\" % counter)\n",
    "                    sys.stdout.flush()\n",
    "                source_ids = [int(x) for x in source.split()]\n",
    "                target_ids = [int(x) for x in target.split()]\n",
    "                target_ids.append(EOS_ID)\n",
    "                for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "                    if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                        data_set[bucket_id].append([source_ids, target_ids])\n",
    "                        break\n",
    "                source, target = source_file.readline(), target_file.readline()\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(session, forward_only):\n",
    "    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "    dtype = tf.float16 if FLAGS_use_fp16 else tf.float32\n",
    "    \n",
    "    model = seq2seq_model.Seq2SeqModel(\n",
    "      FLAGS_en_vocab_size,\n",
    "      FLAGS_fr_vocab_size,\n",
    "      _buckets,\n",
    "      FLAGS_size,\n",
    "      FLAGS_num_layers,\n",
    "      FLAGS_max_gradient_norm,\n",
    "      FLAGS_batch_size,\n",
    "      FLAGS_learning_rate,\n",
    "      FLAGS_learning_rate_decay_factor,\n",
    "      forward_only=forward_only,\n",
    "      dtype=dtype)\n",
    "    \n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS_train_dir)\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Train a chatbot using subtitles data.\"\"\"\n",
    "    \n",
    "    # Prepare WMT data.\n",
    "    print(\"Preparing WMT data in %s\" % FLAGS_data_dir)\n",
    "    en_train, fr_train, en_dev, fr_dev, _, _ = prepare_wmt_data(\n",
    "                    FLAGS_data_dir, FLAGS_en_vocab_size, FLAGS_fr_vocab_size)\n",
    "    #en_train, fr_train, en_dev, fr_dev --> Paths to the four files with enc_trn, dec_trn, enc_tst, dec_tst sentences\n",
    "    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Create model.\n",
    "        print(\"Creating %d layers of %d units.\" % (FLAGS_num_layers, FLAGS_size))\n",
    "        model = create_model(sess, False)\n",
    "\n",
    "        # Read data into buckets and compute their sizes.\n",
    "        print (\"Reading development and training data (limit: %d).\" % FLAGS_max_train_data_size)\n",
    "        dev_set = read_data(en_dev, fr_dev)\n",
    "        train_set = read_data(en_train, fr_train, FLAGS_max_train_data_size)\n",
    "        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "        # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "        # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "        # the size if i-th training bucket, as used later.\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "        # This is the training loop.\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        while True:\n",
    "        \n",
    "            # Choose a bucket according to data distribution. We pick a random number\n",
    "            # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "            # Get a batch and make a step.\n",
    "            start_time = time.time()\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n",
    "            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "            step_time += (time.time() - start_time) / FLAGS_steps_per_checkpoint\n",
    "            loss += step_loss / FLAGS_steps_per_checkpoint\n",
    "            current_step += 1\n",
    "\n",
    "            # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "            if current_step % FLAGS_steps_per_checkpoint == 0:\n",
    "            \n",
    "                # Print statistics for the previous epoch.\n",
    "                perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "                print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "                       \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity))\n",
    "            \n",
    "                # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                previous_losses.append(loss)\n",
    "            \n",
    "                # Save checkpoint and zero timer and loss.\n",
    "                checkpoint_path = os.path.join(FLAGS_train_dir, \"translate.ckpt\")\n",
    "                model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "                step_time, loss = 0.0, 0.0\n",
    "            \n",
    "                # Run evals on development set and print their perplexity.\n",
    "                for bucket_id in xrange(len(_buckets)):\n",
    "                    if len(dev_set[bucket_id]) == 0:\n",
    "                        print(\"  eval: empty bucket %d\" % (bucket_id))\n",
    "                        continue\n",
    "                    encoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\n",
    "                    _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "                    eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
    "                    print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "                \n",
    "                sys.stdout.flush()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "model = create_model(sess, True)\n",
    "model.batch_size = 1  # We decode one sentence at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load vocabularies.\n",
    "en_vocab_path = os.path.join(FLAGS_data_dir, \"vocab%d.en\" % FLAGS_en_vocab_size)\n",
    "fr_vocab_path = os.path.join(FLAGS_data_dir, \"vocab%d.de\" % FLAGS_fr_vocab_size)\n",
    "en_vocab, _ = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "_, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def response(sentence):\n",
    "\n",
    "    # Get token-ids for the input sentence.\n",
    "    token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), en_vocab)\n",
    "    \n",
    "    # Which bucket does it belong to?\n",
    "    bucket_id = len(_buckets) - 1\n",
    "    for i, bucket in enumerate(_buckets):\n",
    "        if bucket[0] >= len(token_ids):\n",
    "            bucket_id = i\n",
    "            break\n",
    "    else:\n",
    "        logging.warning(\"Sentence truncated: %s\", sentence) \n",
    "\n",
    "    # Get a 1-element batch to feed the sentence to the model.\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch({bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "    \n",
    "    # Get output logits for the sentence.\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "    \n",
    "    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    \n",
    "    # If there is an EOS symbol in outputs, cut them at that point.\n",
    "    if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "        \n",
    "    # Print out French sentence corresponding to outputs.\n",
    "    return \" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(response('buenos das'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(response('como ests ?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(response('cuentame algo sobre ti'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(response('cuantos aos tienes ?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(response('quieres estar solo ?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(response('hasta luego'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(response('quien eres?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode():\n",
    "    with tf.Session() as sess:\n",
    "        # Create model and load parameters.\n",
    "        model = create_model(sess, True)\n",
    "        model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "        # Load vocabularies.\n",
    "        en_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                     \"vocab%d.en\" % FLAGS.en_vocab_size)\n",
    "        fr_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.fr\" % FLAGS.fr_vocab_size)\n",
    "        en_vocab, _ = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "        _, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "        # Decode from standard input.\n",
    "        sys.stdout.write(\"> \")\n",
    "        sys.stdout.flush()\n",
    "        sentence = sys.stdin.readline()\n",
    "        while sentence:\n",
    "            # Get token-ids for the input sentence.\n",
    "            token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), en_vocab)\n",
    "            # Which bucket does it belong to?\n",
    "            bucket_id = len(_buckets) - 1\n",
    "            for i, bucket in enumerate(_buckets):\n",
    "                if bucket[0] >= len(token_ids):\n",
    "                    bucket_id = i\n",
    "                    break\n",
    "            else:\n",
    "                logging.warning(\"Sentence truncated: %s\", sentence) \n",
    "\n",
    "            # Get a 1-element batch to feed the sentence to the model.\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "                  {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "            # Get output logits for the sentence.\n",
    "            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                         target_weights, bucket_id, True)\n",
    "            # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "            # If there is an EOS symbol in outputs, cut them at that point.\n",
    "            if data_utils.EOS_ID in outputs:\n",
    "                  outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "            # Print out French sentence corresponding to outputs.\n",
    "            print(\" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\n",
    "            print(\"> \", end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            sentence = sys.stdin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
